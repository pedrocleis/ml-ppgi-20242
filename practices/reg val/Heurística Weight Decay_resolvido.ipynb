{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prática1_Regularização - Heurística Weight Decay_resolvido.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORmyoQkSms+VspgIU7BPjA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sKPfuEnapDnU"},"source":["# <font color=\"darkblue\"> Prática 01: Regularização - Heurística Weight Decay </font>"]},{"cell_type":"markdown","metadata":{"id":"nKa5whanpntS"},"source":["**Objetivos:**\n","\n","\n","*   Testar a Regressão logística com outra base de dados\n","*   Implementar a heurística *weight decay* para regularização da função inferida pelo algoritmo de aprendizagem \n","\n","**Requisitos de execução:**\n","\n","\n","*   Upload dos arquivos *logisticregression.py* e *diabetes.csv*"]},{"cell_type":"markdown","metadata":{"id":"OPCbV-Udr1Pz"},"source":["**Atividade 1:**\n","\n","1. Visitar a base de dados: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n","2. Carregar os dados do arquivo *diabetes.csv* utilizando o pandas.\n","\n","    "]},{"cell_type":"code","metadata":{"id":"ZROjmwlFocyd"},"source":["import pandas as pd \n","\n","diabete = pd.read_csv(\"diabetes.csv\", sep=',')\n","\n","# Pega o cabecalho do arquivo\n","print(diabete.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O73AMlZRtOTU"},"source":["**Atividade 2:**\n","\n","1. Extrair os valores do *DataFrame* pandas e colocar nas variáveis\n"]},{"cell_type":"code","metadata":{"id":"sU9ugDeOtaHd"},"source":["Features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',  'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n","X = diabete[Features].values\n","y = diabete.Outcome.values\n","\n","print(X)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWlIeFootavy"},"source":["**Atividade 3:**\n","\n","1. Separar os dados em conjunto de treinamento e teste\n"]},{"cell_type":"code","metadata":{"id":"1CEvKox1tsWj"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","x_T, x_test, y_T, y_test = train_test_split(X, y, test_size=0.2, random_state=2698)\n","\n","sc = StandardScaler()\n","x_T = sc.fit_transform(x_T)\n","x_test = sc.fit_transform(x_test)\n","\n","print(\"Tamanho treino: \" + str(len(x_T)))\n","print(\"Tamanho teste: \" + str(len(x_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGjfXNZ-tsor"},"source":["**Atividade 4:**\n","\n","1. Utilize a classe LogisticRegression, importada do pacote *sklearn.metrics*, para inferir aprendizado dos dados de treinamento;\n","2. Compute as métricas de aprendizado sobre os dados de teste."]},{"cell_type":"code","metadata":{"id":"jvw7C1Odt3Jb"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","model = LogisticRegression(solver='newton-cg')\n","model.fit(x_T, y_T)\n","\n","print(classification_report(y_test, model.predict(x_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SH_p2JeTmEN"},"source":["**Atividade 5:**\n","\n","1. Utilize a classe LogisticRegression, importada do arquivo *logisticregression.py*, para inferir aprendizado dos dados de treinamento;\n","2. Compute o $E_{in}$ sobre os dados de treino e o $E_{out}$ sobre os dados de teste;\n","2. Compute as métricas de aprendizado sobre os dados de teste."]},{"cell_type":"code","metadata":{"id":"GbnZN9PUQQ0d"},"source":["from sklearn.metrics import classification_report\n","from logisticregression import LogisticRegression\n","\n","lrY = [+1 if value == 1 else -1 for value in y_T]\n","lr_y_test = [+1 if value == 1 else -1 for value in y_test]\n","\n","#Regressão Logistica\n","classifier = LogisticRegression(0.1, 1000, 32)\n","classifier.fit(x_T, lrY)\n","\n","def error(pred, y):\n","    N = len(y)\n","    error = 0\n","    for i in range(N):\n","        if(pred[i] != y[i]):\n","          error += 1\n","    error /= N\n","    return error\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(x_T)\n","print(\"Ein = \" + str(error(pred, lrY)))\n","\n","#Computando o erro fora da amostra (Eout)\n","pred = classifier.predict(x_test)\n","print(\"Eout = \" + str(error(pred, lr_y_test)))\n","\n","#Métricas de aprendizado\n","pred = classifier.predict(x_test)\n","print(classification_report(lr_y_test, pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7eMziL89QAyK"},"source":["Atividade 6:\n","\n","1. Implemente a heurística *WeightDecay* utilizando a penalidade $L_2$"]},{"cell_type":"code","metadata":{"id":"uZXxRdSyQBLX","executionInfo":{"status":"ok","timestamp":1633614058253,"user_tz":180,"elapsed":314,"user":{"displayName":"Gilberto Farias","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08659255523274913012"}}},"source":["from logisticregression import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings(\"error\")\n","\n","class WeightDecay:\n","   \n","    # Infere o vetor w da funçao hipotese\n","    #Executa a heuristica Weigth Decay\n","    def fit(self, _X, _y):\n","        x_train, x_val, y_train, y_val = train_test_split(_X, _y, test_size=0.2, random_state=2698)\n","        \n","        d = x_train.shape[1]\n","        N = x_train.shape[0]\n","        w = np.zeros(d, dtype=np.float128)\n","        \n","        lambdas = sorted([10 ** x for x in range(-5, 5)])\n","        print(lambdas)\n","        lambdas.append(0)\n","        bestEout = 100\n"," \n","        for lam in lambdas:\n","            classifier = LogisticRegression(0.1, 1000, 32, lam)\n","            try:\n","              classifier.fit(x_train, y_train)\n","            except RuntimeWarning:\n","              continue \n","            #Computando o erro quadrático (Eout)\n","            eOut = self.getEout(classifier.predict(x_val), y_val)                \n","            \n","            print(\"LA: \" + str(classifier.lam) + \"  Eval = \" + str(eOut))\n","            if eOut < bestEout:\n","                bestEout = eOut\n","                self.classifier = classifier\n","                    \n","        self.w = self.classifier.w\n","        #print(\"Best Lamb: \" + str(self.classifier.lam))\n","        \n","    #funcao hipotese inferida pela regressa logistica  \n","    def predict_prob(self, X):\n","        return [(1 / (1 + np.exp(-(self.w[0] + self.w[1:].T @ x)))) for x in X]\n","\n","    #Predicao por classificação linear\n","    def predict(self, X):\n","        return [1 if (1 / (1 + np.exp(-(self.w[0] + self.w[1:].T @ x)))) >= 0.5 \n","                else -1 for x in X]\n","\n","    def getW(self):\n","        return self.w\n","\n","    def getRegressionY(self, regressionX, shift=0):\n","        return (-self.w[0]+shift - self.w[1]*regressionX) / self.w[2]\n","    \n","    def getEout(self, pred, y_val):\n","        #Computando o erro quadrático (Eout)\n","        N_val = len(y_val)\n","        eOut = 0\n","        for i in range(N_val):\n","            if(pred[i] != y_val[i]):\n","              eOut += 1 \n","        eOut /= N_val\n","        return eOut"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fyr1aWvJQGbI"},"source":["**Atividade 7:**\n","\n","1. Utilize a classe WeightDecay para inferir aprendizado dos dados de treinamento;\n","2. Compute o $E_{in}$ sobre os dados de treino e o $E_{out}$ sobre os dados de teste;\n","3. Compute as métricas de aprendizado sobre os dados de teste e compare os resutlados."]},{"cell_type":"code","metadata":{"id":"SGqo7qhUQHrY"},"source":["from sklearn.metrics import classification_report\n","\n","#Weigth Decay heuristic\n","classifier = WeightDecay()\n","classifier.fit(x_T, lrY)\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(x_T)\n","print(\"\\nEin = \" + str(error(pred, lrY)))\n","\n","#Computando o erro dentro da amostra (Eout)\n","pred = classifier.predict(x_test)\n","print(\"Eout = \" + str(error(pred, lr_y_test)))\n","\n","#Métricas de aprendizado\n","pred = classifier.predict(x_test)\n","print(classification_report(lr_y_test, pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VI3GaycEKXh-"},"source":["Atividade 8:\n","\n","1. Implementar a validação da Logistic Regression através da classe *GridSearchCV* do pacote *sklearn.model_selection*;\n","2. Imprimir os parâmetros do melhor classificador;\n","3. Imprimir os $E_{in}$ e $E_{out}$  e as métricas de aprendizado.\n","\n","Parâmetros:\n","\n","\n","*   *estimator* : instância do classificador cujos hiperparâmetros serão analisados;\n","*   *cv* : número de divisões do conjunto de treinamento para ser usado na técnica de validação cruzada (10 é um bom valor observado na prática);\n","*   *param_grid* : conjunto de parâmetros a serem combinados durante a fase de validação.\n"]},{"cell_type":"code","metadata":{"id":"elugBPN4DAWo"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","lr = LogisticRegression(solver='newton-cg')\n","\n","# Create the random grid\n","param_grid = {\n","              'C' : np.logspace(-5, 5, 10),\n","              }\n","\n","CV_rf = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 10, verbose=2, n_jobs=-1)\n","\n","# Fit the random search model\n","CV_rf.fit(x_T, y_T)\n","\n","print(CV_rf.best_estimator_)\n","\n","\n","print('Ein: %0.4f' % (1 - accuracy_score(y_T, CV_rf.predict(x_T))))\n","print('Eout: %0.4f' % (1 - accuracy_score(y_test, CV_rf.predict(x_test))))\n","print(classification_report(y_test, CV_rf.predict(x_test)))"],"execution_count":null,"outputs":[]}]}