{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8l8KxMSHPdlD0NBc+i0FA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sKPfuEnapDnU"},"source":["# <font color=\"darkblue\"> Prática 01: Classificação Linear - Regressão Logística </font>"]},{"cell_type":"markdown","metadata":{"id":"nKa5whanpntS"},"source":["**Objetivos:**\n","\n","\n","*   Implementar o modelo linear de *Regressão Logística*\n","*   Comparar seu resultado de classificiação linear com o LRClassifier\n","\n","**Requisitos de execução:**\n","\n","\n","*   Upload dos arquivos *random_input.py*, *linearregression.py* e *lrclassifier.py*"]},{"cell_type":"markdown","metadata":{"id":"OPCbV-Udr1Pz"},"source":["**Atividade 1:**\n","\n","1. Gere $N=100$ pontos aleatórios com rótulos de classificação utilizando a classe RandomInput da Aula 14;\n","\n","    "]},{"cell_type":"code","metadata":{"id":"ZROjmwlFocyd"},"source":["from random_input import RandomInput, draw\n","    \n","N = 100\n","rIN = RandomInput()\n","_X, _y = rIN.get_linear_input(N)\n","    \n","draw(_X, _y, rIN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O73AMlZRtOTU"},"source":["**Atividade 2:**\n","\n","Implemente a *Regressão Logística* com o algoritmo *Gradiente Descendente*:\n","\n","1. $w(0)=0$; $\\eta = 0.1$;\n","2. **Para** $t=0,1,2..,tmax$ **faça**\n","3. $\\quad\\quad g_t=-\\frac{1}{N}\\sum_{n=1}^{N}\\frac{y_nx_n}{1+e^{y_nw^T(t)x_n}}$;\n","4. $\\quad\\quad$ **Se** $(\\lVert g_t \\rVert < \\epsilon)$ : **break**;\n","5. $\\quad\\quad w(t+1) = w(t) - \\eta g_t$;\n","6. Retorne $w$\n"]},{"cell_type":"code","metadata":{"id":"sU9ugDeOtaHd"},"source":["import numpy as np\n","from numpy import linalg as LA\n","import random\n","from random import sample \n","\n","\n","class LogisticRegression_:\n","    def __init__(self, eta=0.1, tmax=1000, bs=1000000):\n","      self.eta = eta\n","      self.tmax = tmax\n","      self.batch_size = bs\n","\n","    # Infere o vetor w da funçao hipotese\n","    #Executa a minimizao do erro de entropia cruzada pelo algoritmo gradiente de descida\n","    def fit(self, _X, _y):\n","        X = np.array(_X)\n","        y = np.array(_y)\n","\n","        N = X.shape[0]\n","        d = X.shape[1]\n","    \n","        \n","    #funcao hipotese inferida pela regressa logistica  \n","    def predict_prob(self, X):\n","\n","    #Predicao por classificação linear\n","    def predict(self, X):\n","    \n","    def getW(self):\n","        return self.w\n","\n","    def getRegressionY(self, regressionX, shift=0):\n","        return (-self.w[0]+shift - self.w[1]*regressionX) / self.w[2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWlIeFootavy"},"source":["**Atividade 3:** \n","\n","1. Utilize o algoritmo de *Regressão Logística* para computar a função hipótese $g(x)=\\theta(w^Tx)$ inferida sobre os pontos aleatórios gerados;\n","2. Plote, em um gráfico de dispersão, a probabilidade inferida pelo algoritmo em cada ponto;\n","3. Utilize a *Regressão Logística* como classificador linear e plote os pontos classificados com bolinha azul(1) e vermelha(0);\n","4. Compute o erro dentro da amostra gerado pela classificação linear do algoritmo de *Regressão Logistica*.\n"]},{"cell_type":"code","metadata":{"id":"1CEvKox1tsWj"},"source":["from matplotlib import pyplot as plt\n","\n","draw(_X, _y, rIN)\n","rx = [ [1, x[0], x[1]] for x in _X]\n","\n","#Executar o ajuste dos dados com Regressao Logistica\n","classifier = LogisticRegression_(0.1, 1000)\n","classifier.fit(rx, _y)\n","\n","#Plota as porcentagens da Regressao Logistica \n","pred = classifier.predict_prob(rx)\n","print(pred)\n","\n","for x, pred_x in zip(_X, pred) :\n","  plt.annotate(\"{:.2f}\".format(pred_x), xy=(x[0], x[1]), xytext=(5,-5), textcoords='offset points')\n","\n","#desenha a reta inferida pela regressao logistica\n","xRL = [-1, +1]\n","yRL = [classifier.getRegressionY(xRL[0]), classifier.getRegressionY(xRL[1])]\n","plt.plot(xRL, yRL, 'y-', label='g(.)')\n","plt.legend(loc='upper right')\n","\n","#plota os pontos classificados pela RL com bolinha azul(+) e vermelha(-)\n","xP = [_X[i][0] for i in range(N) if(pred[i] >= 0.5)]\n","yP = [_X[i][1] for i in range(N) if(pred[i] >= 0.5)]\n","xN = [_X[i][0] for i in range(N) if(pred[i] < 0.5)]\n","yN = [_X[i][1] for i in range(N) if(pred[i] < 0.5)]\n","\n","plt.scatter(xP, yP, color='blue', marker='o', s=20)\n","plt.scatter(xN, yN, color='red', marker='o', s=20)\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(rx)\n","\n","errorIN = 0\n","for pn, yn in zip(pred, _y):\n","  if(pn != yn):\n","    errorIN += 1\n","\n","print(\"Error in : \" + str(errorIN/N * 100) + \"%\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGjfXNZ-tsor"},"source":["**Atividade 4:**\n","\n","1. Reproduza o mesmo experimento para o classificador linear com Regressão Linear (*RCClassifier*);\n","2. Implemente o algoritmo *Gradiente Descendente Estocástico* na classe *LogisticRegression_* e compare os resultados."]},{"cell_type":"code","metadata":{"id":"jvw7C1Odt3Jb"},"source":["from lrclassifier import LRClassifier\n","\n","classifier = LRClassifier()\n","classifier.fit(rx, _y)\n","\n","draw(_X, _y, rIN)\n","\n","#desenha a reta inferida pela regressao linear\n","xRL = [-1, +1]\n","yRL = [classifier.getRegressionY(xRL[0]), classifier.getRegressionY(xRL[1])]\n","plt.plot(xRL, yRL, 'y-', label='g(.)')\n","plt.legend(loc='upper right')\n","\n","#plota os pontos classificados pela RL com bolinha azul(+) e vermelha(-)\n","pred = classifier.predict(rx)\n","xP = [_X[i][0] for i in range(N) if(pred[i] > 0)]\n","yP = [_X[i][1] for i in range(N) if(pred[i] > 0)]\n","xN = [_X[i][0] for i in range(N) if(pred[i] < 0)]\n","yN = [_X[i][1] for i in range(N) if(pred[i] < 0)]\n","\n","plt.scatter(xP, yP, color='blue', marker='o', s=20)\n","plt.scatter(xN, yN, color='red', marker='o', s=20)\n","\n","#Computando o erro dentro da amostra (Ein)\n","eIn = 0\n","for i in range(N):\n","  if(pred[i] != _y[i]):\n","    eIn += 1\n","eIn /= N\n","print(\"Ein = \" + str(eIn * 100) + \"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SH_p2JeTmEN"},"source":["Atividade 5:\n","\n","1. Reproduza o mesmo experimento com a classe LogisticRegression do pacote *sklearn.linear_model*\n","        from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","metadata":{"id":"GbnZN9PUQQ0d"},"source":["from sklearn.linear_model import LogisticRegression\n","draw(_X, _y, rIN)\n","\n","classifier = LogisticRegression(max_iter=1000)\n","classifier.fit(rx, _y) \n","\n","#Plota as porcentagens da Regressao Logistica \n","pred = classifier.predict_proba(rx)\n","#print(pred)\n","\n","#Computando o erro dentro da amostra (Ein)\n","pred = classifier.predict(rx)\n","#print(pred)\n","\n","errorIN = 0\n","for pn, yn in zip(pred, _y):\n","  if(pn != yn):\n","    errorIN += 1\n","\n","print(\"Error in : \" + str(errorIN/N * 100) + \"%\")\n"],"execution_count":null,"outputs":[]}]}